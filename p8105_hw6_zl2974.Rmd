---
title: "Homework 6"
author: "Jeffrey Liang"
date: "11/23/2020"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(readxl)
library(patchwork)
knitr::opts_chunk$set(
  fig.height = 6,
  fig.width = 8,
  message = F,
  echo = T,
  warning = F
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis",
  digits = 3
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

# Problem 1

Read in the data.
```{r}
homicide_df = 
  read_csv("data/data-homicides-master/homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    ),
    resolved = forcats::fct_relevel(resolved,"unsolved"),
    victim_age = as.numeric(victim_age),
    victim_sex = as.factor(victim_sex)
  ) %>% 
  select(-city,-state,-victim_last,-victim_first,-lon,-lat) %>% 
  filter(!str_detect(city_state,"Tulsa|Dallas|Phoenix|Kansas"),
         victim_race %in% c("White","Black"),
         victim_sex %in% c("Male","Female")) %>% 
  nest(c(-city_state))
```

Do linear regression
```{r}
homicide_df =
  homicide_df %>%
  mutate(log = map(
    .x = data,
    ~ glm(
      resolved ~ victim_age + victim_sex + victim_race,
      data = .x,
      family = binomial()
    )
  ),
  result  = map(log, broom::tidy)) %>% 
  select(-data) %>% 
  unnest(result) %>% 
  mutate(ci = map2(.x = log,
                               .y = term,
                               ~confint(.x,.y))) %>% 
  select(city_state,term,estimate,ci) %>% 
  filter(term == "victim_raceWhite")

homicide_df %>% 
  head() %>% 
  knitr::kable()
```


# Problem 2

Import data
```{r}
birthweight =
  read_csv(here::here("data/birthweight.csv")) %>%
  janitor::clean_names() %>%
  mutate(
    across(
      c("frace", "mrace"),
      ~ case_when(
        .x == 1 ~ "White",
        .x == 2 ~ "Black",
        .x == 3 ~ "Asian",
        .x == 4 ~ "Puero Rican",
        .x == 8 ~ "Other",
        .x == 9 ~ " Unknown"
      )
    ),
    across(where(is.character), as.factor),
    malform = case_when(malform == 0 ~ "absent",
                        malform == 1 ~ "present"),
    malform = as.factor(malform),
    babysex = case_when(babysex == 1 ~ "male",
                        babysex == 2 ~ "female") %>% as.factor()
  )


birthweight %>% skimr::skim_without_charts()
```

We don't know what we interest in, so I just assume to predict birthweight, so 
let first make our data in to cross validation partitions.
```{r cache=T}
md_step =
  glm(bwt ~ ., data = birthweight) %>% 
  step(direction = "both",
                    scope = list(lower = . ~ 1,
                                 upper = . ~ . ^ 2),
                    trace = 0)

summary(md_step)

md_step =
  terms(md_step) %>% 
  attr("term.labels") %>% 
  str_c(.,collapse = "+") %>% 
  str_c("bwt~",.) %>% 
  as.formula()

set.seed(1)
birthweight_cv =
  modelr::crossv_mc(birthweight, 100, id = "id") %>%
  mutate(
    md_step =
      map(.x = train,  ~ do.call("glm", list(bwt ~ ., data = .x))),
    md_step = map(.x = md_step,
                  ~ step(
                    .x,
                    direction = "both",
                    scope = list(lower = . ~ 1,
                                 upper = . ~ . ^ 2),
                    trace = 0
                  )),
    #is the stepwise model all alike across all cv train?
    md_eg1 =
      map(.x = train,  ~ lm(bwt ~ blength + gaweeks, data = .x)),
    md_eg2 =
      map(.x = train,  ~ lm(bwt ~ blength * bhead * babysex, data = .x)),
    #Fitted the model, now how to verify the prediction?
    rmse_step =
      map2_dbl(.x = md_step,
               .y = test,
               ~ modelr::rmse(.x, .y)),
    rmse_eg1 =
      map2_dbl(.x = md_eg1,
               .y = test,
               ~ modelr::rmse(.x, .y)),
    rmse_eg2 =
      map2_dbl(.x = md_eg2,
               .y = test,
               ~ modelr::rmse(.x, .y))
  )

birthweight_cv %>%
  mutate(across(starts_with("rmse"), as.numeric)) %>%
  pivot_longer(
    starts_with("rmse"),
    values_to = "rmse",
    names_to =  "model",
    names_prefix = "rmse_"
  ) %>%
  ggplot(aes(x = model, y = rmse)) +
  geom_violin()
```
\ My model is generate via both-way stepwise variable selection. The second example 
model has lower rmse compared to the first model, and my model has lower rmse comapred
 to both example model. 
 
 # Problem 3
 
 Load data
```{r noaa, cache=T}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

 Look at the distribution
```{r}
weather_df %>% 
  ggplot(aes(x = tmin, y = tmax))+
  geom_point(alpha = 0.4)+
  geom_smooth(method = "lm")
```
 
Produce bootstrap sample
```{r bootstraping, cache=T}
set.seed(1)
weather_bt =
  modelr::bootstrap(weather_df,
                    n = 1000,
                    id = "id") %>%
  mutate(
    strap = map(strap, as_tibble),
    md =
      map(.x = strap,
          ~ lm(tmax ~ tmin, data = .x)),
    r_square =
      map2_dbl(.x = md,
               .y = strap,
               ~ modelr::rsquare(.x, .y)),
    md = map(md, broom::tidy)
  ) %>%
  unnest(md) %>%
  select(id, term, estimate, std_error = std.error, r_square)

weather_bt %>% head() %>% knitr::kable()
```
Examine Bootstrap result
```{r boostrap_result}
weather_bt %>% 
  distinct(id,r_square) %>% 
  ggplot(aes(x = r_square))+
  geom_density()

weather_bt %>% 
  select(id,term,estimate) %>% 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) %>% 
  janitor::clean_names() %>% 
  mutate(
    log_beta =
      log(tmin*intercept)
  ) %>% 
  ggplot(aes(x = log_beta))+
  geom_density()
```

\ The r^2 follows approximately normal distribution. By central limit theorem 
the sampling distribution of R^2 will follow normal distribution with mean equal
 to true R^2.
 
\ The log(beta0hat and the beta1hat) follows an approximately normal distribution, follows the central limit theorem.
